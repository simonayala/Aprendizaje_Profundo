{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctico 2 - Redes en escalera avanzadas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/simon/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/simon/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from IPython.display import SVG\n",
    "from gensim import corpora\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "\n",
    "nltk.download([\"punkt\", \"stopwords\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Fee</th>\n",
       "      <th>State</th>\n",
       "      <th>Description</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "      <th>PID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>41326</td>\n",
       "      <td>Nibble is a 3+ month old ball of cuteness. He ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>41401</td>\n",
       "      <td>Good guard dog, very alert, active, obedience ...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41326</td>\n",
       "      <td>This handsome yet cute boy is up for adoption....</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Age  Breed1  Breed2  Gender  Color1  Color2  Color3  MaturitySize  \\\n",
       "0     2    3     299       0       1       1       7       0             1   \n",
       "1     1    4     307       0       2       1       2       0             2   \n",
       "2     1    1     307       0       1       1       0       0             2   \n",
       "\n",
       "   FurLength  Vaccinated  Dewormed  Sterilized  Health  Quantity  Fee  State  \\\n",
       "0          1           2         2           2       1         1  100  41326   \n",
       "1          1           1         1           2       1         1  150  41401   \n",
       "2          1           2         2           2       1         1    0  41326   \n",
       "\n",
       "                                         Description  AdoptionSpeed  PID  \n",
       "0  Nibble is a 3+ month old ball of cuteness. He ...              2    0  \n",
       "1  Good guard dog, very alert, active, obedience ...              2    3  \n",
       "2  This handsome yet cute boy is up for adoption....              2    4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(os.path.join(DATA_DIRECTORY, 'train.csv'))\n",
    "\n",
    "target_col = 'AdoptionSpeed'\n",
    "nlabels = dataset[target_col].unique().shape[0]\n",
    "\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproceso del texto para agregarlo como feature (manejo de secuencias)\n",
    "\n",
    "Al utilizar la columna de descripcion como un feature, es necesario preprocesarlo para poder insertarlo a la red neuronal de una forma eficiente. En este caso en particular se procesara en una red recurrente y luego se concatenaran los resultados de esta con el resto de los atributos (Breed1, Fee, Age, Gender, Color1).\n",
    "\n",
    "\n",
    "\n",
    "### Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se eliminan las stopwords y se pasan las palabras a minuscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SW = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize_description(description):\n",
    "    return [w.lower() for w in word_tokenize(description, language=\"english\") if w.lower() not in SW]\n",
    "\n",
    "# Fill the null values with the empty string to avoid errors with NLTK tokenization\n",
    "dataset[\"TokenizedDescription\"] = dataset[\"Description\"].fillna(value=\"\").apply(tokenize_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tamaño de las descripciones\n",
    "\n",
    "Un punto importante a tener en cuenta es que las descripciones tienen tamaño variable, y esto no es compatible con los algoritmos de aprendizaje automático. Por lo que hay que llevar las secuencias a un tamaño uniforme.\n",
    "\n",
    "Para definir dicho tamaño uniforme, es útil mirar qué tamaños mínimos, máximos y medios manejan las descripciones y a partir de esto establecer el tamaño máximo de la secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    10582.000000\n",
      "mean        44.419486\n",
      "std         48.465910\n",
      "min          0.000000\n",
      "25%         16.000000\n",
      "50%         31.000000\n",
      "75%         55.000000\n",
      "max        803.000000\n",
      "Name: TokenizedDescription, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset[\"TokenizedDescription\"].apply(len).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el 75% de las secuencias tienen 55 palabras o menos, selecccionamos el tamaño máximo de las secuencia en 55 palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LEN = 55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera un diccionario del vocabulario y se limita el tamaño de este a las 10000 palabras mas comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = corpora.Dictionary(dataset[\"TokenizedDescription\"])\n",
    "vocabulary.filter_extremes(no_below=1, no_above=1.0, keep_n=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings (GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza un archivo de GloVe (que es un vector de representacion de palabras proveniente de un algoritmo de aprendizaje no supervisado) para asignar a cada palabra una representacion vectorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7897 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "with open(\"glove.6B.100d.txt\", \"r\", encoding=\"utf8\") as fh:\n",
    "    for line in fh:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if word in vocabulary.token2id:  # Only use the embeddings of words in our vocabulary\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found {} word vectors.\".format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de los datasets\n",
    "\n",
    "En este practico utilizaremos las variables Gender, Color1, Breed1, Age, Fee y Description. En cuando al preprocesamiento, a Gender y Color1 se le aplicara el metodo de one hot encoding, a Breed1 se le aplicara un embedding y a Age y Fee un escalado minmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's important to always use the same one-hot length\n",
    "one_hot_columns = {\n",
    "    one_hot_col: dataset[one_hot_col].max()\n",
    "    for one_hot_col in ['Gender', 'Color1']\n",
    "}\n",
    "embedded_columns = {\n",
    "    embedded_col: dataset[embedded_col].max() + 1\n",
    "    for embedded_col in ['Breed1']\n",
    "}\n",
    "numeric_columns = ['Age', 'Fee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generador del conjunto de datos\n",
    "\n",
    "Dada la naturaleza de los datos de texto, y que estos representan una secuencia de datos (que se da luego a una red recurrente o convolucional), en este caso no crearemos los datasets de antemano, sino que los generaremos a medida que el algoritmo de entrenamiento los pida. \n",
    "\n",
    "En particular, es porque las secuencias de texto pueden no tener el mismo tamaño (las oraciones tienen diferente cantidad de palabras), pero para que los modelos de redes las acepten, necesitamos rellenarlas (*padding*) de manera que todas tengan el mismo tamaño.\n",
    "\n",
    "En este paso también vamos a truncar aquellas secuencias de descripciones con más de `MAX_SEQUENCE_LEN` palabras, de manera que al hacer uso de `padded_batch` no lance un error al encontrarse con secuencias de tamaño mayor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de crear los dataset es necesario aplicarle el escalado minmax a las variables numericas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Age': <tf.Tensor: id=20150, shape=(1,), dtype=float32, numpy=array([0.01176471], dtype=float32)>,\n",
      " 'Breed1': <tf.Tensor: id=20151, shape=(1,), dtype=int32, numpy=array([299], dtype=int32)>,\n",
      " 'Fee': <tf.Tensor: id=20152, shape=(1,), dtype=float32, numpy=array([0.03333334], dtype=float32)>,\n",
      " 'description': <tf.Tensor: id=20153, shape=(42,), dtype=int32, numpy=\n",
      "array([23,  2, 20, 24,  4, 10,  1, 11, 26,  1, 27,  9,  6, 21,  3,  8, 15,\n",
      "       22, 33,  7, 13, 30,  1, 29, 18, 17,  1, 12, 31, 14,  5,  6, 16,  1,\n",
      "       19, 28, 25, 32, 23,  0,  5,  1], dtype=int32)>,\n",
      " 'direct_features': <tf.Tensor: id=20154, shape=(10,), dtype=float32, numpy=array([1., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)>}\n",
      "<tf.Tensor: id=20155, shape=(5,), dtype=int32, numpy=array([0, 0, 1, 0, 0], dtype=int32)>\n",
      "\n",
      "{'Age': <tf.Tensor: id=20156, shape=(1,), dtype=float32, numpy=array([0.01568628], dtype=float32)>,\n",
      " 'Breed1': <tf.Tensor: id=20157, shape=(1,), dtype=int32, numpy=array([307], dtype=int32)>,\n",
      " 'Fee': <tf.Tensor: id=20158, shape=(1,), dtype=float32, numpy=array([0.05], dtype=float32)>,\n",
      " 'description': <tf.Tensor: id=20159, shape=(24,), dtype=int32, numpy=\n",
      "array([41, 42, 40, 35, 37, 35, 36, 35, 45, 50, 41, 44, 35, 46, 38, 48, 39,\n",
      "       47, 15, 43, 35, 49, 34, 34], dtype=int32)>,\n",
      " 'direct_features': <tf.Tensor: id=20160, shape=(10,), dtype=float32, numpy=array([0., 1., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)>}\n",
      "<tf.Tensor: id=20161, shape=(5,), dtype=int32, numpy=array([0, 0, 1, 0, 0], dtype=int32)>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dataset_generator(ds, test_data=False):\n",
    "    for _, row in ds.iterrows():\n",
    "        instance = {}\n",
    "        \n",
    "        # One hot encoded features\n",
    "        instance[\"direct_features\"] = np.hstack([\n",
    "            tf.keras.utils.to_categorical(row[one_hot_col] - 1, max_value)\n",
    "            for one_hot_col, max_value in one_hot_columns.items()\n",
    "        ])\n",
    "\n",
    "        \n",
    "        # Embedded features\n",
    "        for embedded_col in embedded_columns:\n",
    "            instance[embedded_col] = [row[embedded_col]]\n",
    "        \n",
    "        dataset[numeric_columns]=sklearn.preprocessing.minmax_scale(dataset[numeric_columns], feature_range=(0, 1), axis=0)\n",
    "        \n",
    "        # Numeric features\n",
    "        for numeric_col in numeric_columns:\n",
    "            instance[numeric_col] = [row[numeric_col]]\n",
    "        \n",
    "        # Document to indices for text data, truncated at MAX_SEQUENCE_LEN words\n",
    "        instance[\"description\"] = vocabulary.doc2idx(\n",
    "            row[\"TokenizedDescription\"],\n",
    "            unknown_word_index=len(vocabulary)\n",
    "        )[:MAX_SEQUENCE_LEN]\n",
    "        \n",
    "        # One hot encoded target for categorical crossentropy\n",
    "        if not test_data:\n",
    "            target = tf.keras.utils.to_categorical(row[target_col], nlabels)\n",
    "            yield instance, target\n",
    "        else:\n",
    "            yield instance\n",
    "\n",
    "# Set output types of the generator (for numeric types check the type is valid)\n",
    "instance_types = {\n",
    "    \"direct_features\": tf.float32,\n",
    "    \"description\": tf.int32,\n",
    "\n",
    "}\n",
    "\n",
    "for embedded_col in embedded_columns:\n",
    "    instance_types[embedded_col] = tf.int32\n",
    "\n",
    "for numeric_col in numeric_columns:\n",
    "    instance_types[numeric_col] = tf.float32\n",
    "\n",
    "tf_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: dataset_generator(dataset),\n",
    "    output_types=(instance_types, tf.int32)\n",
    ")\n",
    "\n",
    "for data, target in tf_dataset.take(2):\n",
    "    pprint(data)\n",
    "    pprint(target)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de entrenamiento y validación\n",
    "\n",
    "Ya generado el conjunto de datos base, tenemos que dividirlo en entrenamiento y validación. Además, como vamos a utilizar algunos datos que forman secuencias, los lotes (*batches*) de datos deben estar \"rellenados\" (*padded_batch*). \n",
    "\n",
    "Si bien rellenaremos \"todos\" los atributos, en la práctica el único que efectivamente se rellenará es el de *description* pues es el único con tamaños distintos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = int(dataset.shape[0] * 0.8)\n",
    "DEV_SIZE = dataset.shape[0] - TRAIN_SIZE\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "shuffled_dataset = tf_dataset.shuffle(TRAIN_SIZE + DEV_SIZE, seed=42)\n",
    "\n",
    "# Pad the datasets to the max value for all the \"non sequence\" features\n",
    "padding_shapes = (\n",
    "    {k: [-1] for k in [\"direct_features\"] + list(embedded_columns.keys())+list([(numeric_columns)[0]])+list([(numeric_columns)[1]])},\n",
    "    [-1]\n",
    ")\n",
    "\n",
    "# Pad to MAX_SEQUENCE_LEN for sequence features\n",
    "padding_shapes[0][\"description\"] = [MAX_SEQUENCE_LEN]\n",
    "\n",
    "# Pad values are irrelevant for non padded data\n",
    "\n",
    "\n",
    "padding_values = (\n",
    "    {k: 0 for k in list(embedded_columns.keys())},\n",
    "    0\n",
    ")\n",
    "\n",
    "padding_values[0][numeric_columns[0]] = np.float32(0)\n",
    "padding_values[0][numeric_columns[1]] = np.float32(0)\n",
    "\n",
    "\n",
    "# Padding value for direct features should be a float\n",
    "padding_values[0][\"direct_features\"] = np.float32(0)\n",
    "\n",
    "# Padding value for sequential features is the vocabulary length + 1\n",
    "padding_values[0][\"description\"] = len(vocabulary) + 1\n",
    "\n",
    "train_dataset = shuffled_dataset.skip(DEV_SIZE)\\\n",
    "    .padded_batch(BATCH_SIZE, padded_shapes=padding_shapes, padding_values=padding_values)\n",
    "\n",
    "dev_dataset = shuffled_dataset.take(DEV_SIZE)\\\n",
    "    .padded_batch(BATCH_SIZE, padded_shapes=padding_shapes, padding_values=padding_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo el modelo\n",
    "\n",
    "\n",
    "\n",
    "### Matriz de embeddings de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_DIM = 100  # Given by the model (in this case glove.6B.100d)\n",
    "\n",
    "embedding_matrix = np.zeros((len(vocabulary) + 2, 100))\n",
    "\n",
    "for widx, word in vocabulary.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[widx] = embedding_vector\n",
    "    else:\n",
    "        # Random normal initialization for words without embeddings\n",
    "        embedding_matrix[widx] = np.random.normal(size=(100,))  \n",
    "\n",
    "# Random normal initialization for unknown words\n",
    "embedding_matrix[len(vocabulary)] = np.random.normal(size=(100,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiendo los inputs del modelo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se generan las distintas \"capas\" de input de la red, en las que se ingresaran los distintos tipos de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding embedding of size 77 for layer Breed1\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Add one input and one embedding for each embedded column\n",
    "embedding_layers = []\n",
    "inputs = []\n",
    "for embedded_col, max_value in embedded_columns.items():\n",
    "    input_layer = tf.keras.layers.Input(shape=(1,), name=embedded_col)\n",
    "    inputs.append(input_layer)\n",
    "    # Define the embedding layer\n",
    "    embedding_size = int(max_value / 4)\n",
    "    embedding_layers.append(\n",
    "        tf.squeeze(\n",
    "            tf.keras.layers.Embedding(\n",
    "                input_dim=max_value, \n",
    "                output_dim=embedding_size\n",
    "            )(input_layer), \n",
    "            axis=-2\n",
    "        )\n",
    "    )\n",
    "    print('Adding embedding of size {} for layer {}'.format(embedding_size, embedded_col))\n",
    "\n",
    "# Add the direct features already calculated\n",
    "direct_features_input = tf.keras.layers.Input(\n",
    "    shape=(sum(one_hot_columns.values()),), \n",
    "    name='direct_features'\n",
    ")\n",
    "inputs.append(direct_features_input)\n",
    "\n",
    "\n",
    "# Add the direct features already calculated\n",
    "numerical_input = tf.keras.layers.Input(\n",
    "    shape=(1,), \n",
    "    name='Age'\n",
    ")\n",
    "inputs.append(numerical_input)\n",
    "\n",
    "\n",
    "numerical_input1 = tf.keras.layers.Input(\n",
    "    shape=(1,), \n",
    "    name='Fee'\n",
    ")\n",
    "inputs.append(numerical_input1)\n",
    "\n",
    "# Word embedding layer\n",
    "description_input = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LEN,), name=\"description\")\n",
    "inputs.append(description_input)\n",
    "\n",
    "word_embeddings_layer = tf.keras.layers.Embedding(\n",
    "    embedding_matrix.shape[0],\n",
    "    EMBEDDINGS_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LEN,\n",
    "    trainable=False,\n",
    "    name=\"word_embedding\"\n",
    ")(description_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiendo la red que trabajará con el texto\n",
    "\n",
    "Antes de generar el *feature map* final entre los inputs y las clases, tenemos que generar el *feature map* de las secuencias de texto, para ello utilizaremos una red neuronal recurrente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Create a NN (CNN or RNN) for the description input (replace the next)\n",
    "DESCRIPTION_FEATURES_LAYER_SIZE = 512\n",
    "\n",
    "\n",
    "#model.add(layers.LSTM(128))\n",
    "#model.add(SimpleRNN(hidden_neurons, return_sequences=False)) \n",
    "description_features=tf.keras.layers.SimpleRNN(256)(word_embeddings_layer)\n",
    "description_features = tf.keras.layers.Flatten()(description_features)  # This is a simple concatenation\n",
    "description_features = tf.keras.layers.Dense(\n",
    "    units=DESCRIPTION_FEATURES_LAYER_SIZE, \n",
    "    activation=\"relu\", \n",
    "    name=\"description_features\")(description_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiendo el *feature map* final de la red\n",
    "\n",
    "Ahora que tenemos nuestra representación de las descripciones, pasamos a combinarlo con los demás features en la última parte de nuestra red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_LAYER_SIZE = 128\n",
    "\n",
    "feature_map = tf.keras.layers.Concatenate(name=\"feature_map\")(\n",
    "    embedding_layers + [description_features, direct_features_input,numerical_input,numerical_input1]\n",
    ")\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dense(HIDDEN_LAYER_SIZE, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(feature_map)\n",
    "hidden_layer=tf.keras.layers.BatchNormalization()(hidden_layer)\n",
    "hidden_layer=tf.keras.layers.Dropout(0.3)(hidden_layer)\n",
    "hidden_layer = tf.keras.layers.Dense(HIDDEN_LAYER_SIZE/2, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(hidden_layer)\n",
    "hidden_layer=tf.keras.layers.BatchNormalization()(hidden_layer)\n",
    "hidden_layer=tf.keras.layers.Dropout(0.3)(hidden_layer)\n",
    "\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(nlabels, activation=\"softmax\", name=\"output\")(hidden_layer)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=[output_layer], name=\"amazing_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilando y visualizando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"amazing_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "description (InputLayer)        [(None, 55)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 55, 100)      1000200     description[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Breed1 (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "simple_rnn (SimpleRNN)          (None, 256)          91392       word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 77)        23716       Breed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 256)          0           simple_rnn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorFlow [(None, 77)]         0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "description_features (Dense)    (None, 512)          131584      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "direct_features (InputLayer)    [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Age (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Fee (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feature_map (Concatenate)       (None, 601)          0           tf_op_layer_Squeeze[0][0]        \n",
      "                                                                 description_features[0][0]       \n",
      "                                                                 direct_features[0][0]            \n",
      "                                                                 Age[0][0]                        \n",
      "                                                                 Fee[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          77056       feature_map[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128)          512         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64)           256         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 5)            325         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,333,297\n",
      "Trainable params: 332,713\n",
      "Non-trainable params: 1,000,584\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='nadam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"767pt\" viewBox=\"0.00 0.00 882.00 921.00\" width=\"735pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 917)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-917 878,-917 878,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140257469017056 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140257469017056</title>\n",
       "<polygon fill=\"none\" points=\"19,-876.5 19,-912.5 163,-912.5 163,-876.5 19,-876.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"91\" y=\"-890.8\">description: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140257469017392 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140257469017392</title>\n",
       "<polygon fill=\"none\" points=\"0,-803.5 0,-839.5 182,-839.5 182,-803.5 0,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"91\" y=\"-817.8\">word_embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 140257469017056&#45;&gt;140257469017392 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140257469017056-&gt;140257469017392</title>\n",
       "<path d=\"M91,-876.4551C91,-868.3828 91,-858.6764 91,-849.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"94.5001,-849.5903 91,-839.5904 87.5001,-849.5904 94.5001,-849.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257757447392 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140257757447392</title>\n",
       "<polygon fill=\"none\" points=\"14.5,-730.5 14.5,-766.5 167.5,-766.5 167.5,-730.5 14.5,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"91\" y=\"-744.8\">simple_rnn: SimpleRNN</text>\n",
       "</g>\n",
       "<!-- 140257469017392&#45;&gt;140257757447392 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140257469017392-&gt;140257757447392</title>\n",
       "<path d=\"M91,-803.4551C91,-795.3828 91,-785.6764 91,-776.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"94.5001,-776.5903 91,-766.5904 87.5001,-776.5904 94.5001,-776.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257469091176 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140257469091176</title>\n",
       "<polygon fill=\"none\" points=\"256.5,-730.5 256.5,-766.5 379.5,-766.5 379.5,-730.5 256.5,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"318\" y=\"-744.8\">Breed1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140257469075808 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140257469075808</title>\n",
       "<polygon fill=\"none\" points=\"244,-657.5 244,-693.5 392,-693.5 392,-657.5 244,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"318\" y=\"-671.8\">embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 140257469091176&#45;&gt;140257469075808 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140257469091176-&gt;140257469075808</title>\n",
       "<path d=\"M318,-730.4551C318,-722.3828 318,-712.6764 318,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"321.5001,-703.5903 318,-693.5904 314.5001,-703.5904 321.5001,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257757447672 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140257757447672</title>\n",
       "<polygon fill=\"none\" points=\"43,-657.5 43,-693.5 139,-693.5 139,-657.5 43,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"91\" y=\"-671.8\">flatten: Flatten</text>\n",
       "</g>\n",
       "<!-- 140257757447392&#45;&gt;140257757447672 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140257757447392-&gt;140257757447672</title>\n",
       "<path d=\"M91,-730.4551C91,-722.3828 91,-712.6764 91,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"94.5001,-703.5903 91,-693.5904 87.5001,-703.5904 94.5001,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257256725416 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140257256725416</title>\n",
       "<polygon fill=\"none\" points=\"192.5,-584.5 192.5,-620.5 443.5,-620.5 443.5,-584.5 192.5,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"318\" y=\"-598.8\">tf_op_layer_Squeeze: TensorFlowOpLayer</text>\n",
       "</g>\n",
       "<!-- 140257469075808&#45;&gt;140257256725416 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140257469075808-&gt;140257256725416</title>\n",
       "<path d=\"M318,-657.4551C318,-649.3828 318,-639.6764 318,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"321.5001,-630.5903 318,-620.5904 314.5001,-630.5904 321.5001,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257469088992 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140257469088992</title>\n",
       "<polygon fill=\"none\" points=\"7.5,-584.5 7.5,-620.5 174.5,-620.5 174.5,-584.5 7.5,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"91\" y=\"-598.8\">description_features: Dense</text>\n",
       "</g>\n",
       "<!-- 140257757447672&#45;&gt;140257469088992 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140257757447672-&gt;140257469088992</title>\n",
       "<path d=\"M91,-657.4551C91,-649.3828 91,-639.6764 91,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"94.5001,-630.5903 91,-620.5904 87.5001,-630.5904 94.5001,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257256724856 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>140257256724856</title>\n",
       "<polygon fill=\"none\" points=\"464.5,-511.5 464.5,-547.5 623.5,-547.5 623.5,-511.5 464.5,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"544\" y=\"-525.8\">feature_map: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140257256725416&#45;&gt;140257256724856 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140257256725416-&gt;140257256724856</title>\n",
       "<path d=\"M373.8652,-584.4551C405.511,-574.2332 445.2698,-561.3907 478.3933,-550.6916\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"479.5542,-553.9947 487.9942,-547.5904 477.4025,-547.3336 479.5542,-553.9947\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257469088992&#45;&gt;140257256724856 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>140257469088992-&gt;140257256724856</title>\n",
       "<path d=\"M174.7043,-585.4887C177.5011,-584.9776 180.2723,-584.4799 183,-584 275.1143,-567.7951 380.9779,-552.1642 454.1361,-541.8295\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"454.8231,-545.2674 464.2372,-540.4068 453.8468,-538.3358 454.8231,-545.2674\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257469091064 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>140257469091064</title>\n",
       "<polygon fill=\"none\" points=\"462,-584.5 462,-620.5 626,-620.5 626,-584.5 462,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"544\" y=\"-598.8\">direct_features: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140257469091064&#45;&gt;140257256724856 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>140257469091064-&gt;140257256724856</title>\n",
       "<path d=\"M544,-584.4551C544,-576.3828 544,-566.6764 544,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"547.5001,-557.5903 544,-547.5904 540.5001,-557.5904 547.5001,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257469077376 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>140257469077376</title>\n",
       "<polygon fill=\"none\" points=\"644.5,-584.5 644.5,-620.5 751.5,-620.5 751.5,-584.5 644.5,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"698\" y=\"-598.8\">Age: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140257469077376&#45;&gt;140257256724856 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>140257469077376-&gt;140257256724856</title>\n",
       "<path d=\"M659.9326,-584.4551C639.2015,-574.628 613.3611,-562.3789 591.3406,-551.9407\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"592.6986,-548.7111 582.1632,-547.5904 589.7002,-555.0365 592.6986,-548.7111\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257469077712 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>140257469077712</title>\n",
       "<polygon fill=\"none\" points=\"770,-584.5 770,-620.5 874,-620.5 874,-584.5 770,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"822\" y=\"-598.8\">Fee: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140257469077712&#45;&gt;140257256724856 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>140257469077712-&gt;140257256724856</title>\n",
       "<path d=\"M769.805,-586.489C766.8302,-585.633 763.8795,-584.7977 761,-584 717.7722,-572.0241 669.3882,-559.7629 629.2959,-549.9107\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"630.1011,-546.5045 619.5554,-547.5243 628.4353,-553.3034 630.1011,-546.5045\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257256726424 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>140257256726424</title>\n",
       "<polygon fill=\"none\" points=\"499.5,-438.5 499.5,-474.5 588.5,-474.5 588.5,-438.5 499.5,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"544\" y=\"-452.8\">dense: Dense</text>\n",
       "</g>\n",
       "<!-- 140257256724856&#45;&gt;140257256726424 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>140257256724856-&gt;140257256726424</title>\n",
       "<path d=\"M544,-511.4551C544,-503.3828 544,-493.6764 544,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"547.5001,-484.5903 544,-474.5904 540.5001,-484.5904 547.5001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257256725192 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>140257256725192</title>\n",
       "<polygon fill=\"none\" points=\"421,-365.5 421,-401.5 667,-401.5 667,-365.5 421,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"544\" y=\"-379.8\">batch_normalization: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140257256726424&#45;&gt;140257256725192 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>140257256726424-&gt;140257256725192</title>\n",
       "<path d=\"M544,-438.4551C544,-430.3828 544,-420.6764 544,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"547.5001,-411.5903 544,-401.5904 540.5001,-411.5904 547.5001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257256793536 -->\n",
       "<g class=\"node\" id=\"node15\">\n",
       "<title>140257256793536</title>\n",
       "<polygon fill=\"none\" points=\"488.5,-292.5 488.5,-328.5 599.5,-328.5 599.5,-292.5 488.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"544\" y=\"-306.8\">dropout: Dropout</text>\n",
       "</g>\n",
       "<!-- 140257256725192&#45;&gt;140257256793536 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>140257256725192-&gt;140257256793536</title>\n",
       "<path d=\"M544,-365.4551C544,-357.3828 544,-347.6764 544,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"547.5001,-338.5903 544,-328.5904 540.5001,-338.5904 547.5001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257256164432 -->\n",
       "<g class=\"node\" id=\"node16\">\n",
       "<title>140257256164432</title>\n",
       "<polygon fill=\"none\" points=\"493,-219.5 493,-255.5 595,-255.5 595,-219.5 493,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"544\" y=\"-233.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140257256793536&#45;&gt;140257256164432 -->\n",
       "<g class=\"edge\" id=\"edge15\">\n",
       "<title>140257256793536-&gt;140257256164432</title>\n",
       "<path d=\"M544,-292.4551C544,-284.3828 544,-274.6764 544,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"547.5001,-265.5903 544,-255.5904 540.5001,-265.5904 547.5001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257256163312 -->\n",
       "<g class=\"node\" id=\"node17\">\n",
       "<title>140257256163312</title>\n",
       "<polygon fill=\"none\" points=\"414,-146.5 414,-182.5 674,-182.5 674,-146.5 414,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"544\" y=\"-160.8\">batch_normalization_1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140257256164432&#45;&gt;140257256163312 -->\n",
       "<g class=\"edge\" id=\"edge16\">\n",
       "<title>140257256164432-&gt;140257256163312</title>\n",
       "<path d=\"M544,-219.4551C544,-211.3828 544,-201.6764 544,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"547.5001,-192.5903 544,-182.5904 540.5001,-192.5904 547.5001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257255886688 -->\n",
       "<g class=\"node\" id=\"node18\">\n",
       "<title>140257255886688</title>\n",
       "<polygon fill=\"none\" points=\"481.5,-73.5 481.5,-109.5 606.5,-109.5 606.5,-73.5 481.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"544\" y=\"-87.8\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 140257256163312&#45;&gt;140257255886688 -->\n",
       "<g class=\"edge\" id=\"edge17\">\n",
       "<title>140257256163312-&gt;140257255886688</title>\n",
       "<path d=\"M544,-146.4551C544,-138.3828 544,-128.6764 544,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"547.5001,-119.5903 544,-109.5904 540.5001,-119.5904 547.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140257255989032 -->\n",
       "<g class=\"node\" id=\"node19\">\n",
       "<title>140257255989032</title>\n",
       "<polygon fill=\"none\" points=\"498,-.5 498,-36.5 590,-36.5 590,-.5 498,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"544\" y=\"-14.8\">output: Dense</text>\n",
       "</g>\n",
       "<!-- 140257255886688&#45;&gt;140257255989032 -->\n",
       "<g class=\"edge\" id=\"edge18\">\n",
       "<title>140257255886688-&gt;140257255989032</title>\n",
       "<path d=\"M544,-73.4551C544,-65.3828 544,-55.6764 544,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"547.5001,-46.5903 544,-36.5904 540.5001,-46.5904 547.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(tf.keras.utils.model_to_dot(model, dpi=60).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando el modelo\n",
    "\n",
    "Para entrenar el modelo es igual al caso anterior, ya generados el conjunto de datos correspondiente. Lo entrenamos con ayuda de `mlflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "67/67 [==============================] - 25s 369ms/step - loss: 2.3316 - accuracy: 0.2169\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 17s 253ms/step - loss: 2.0474 - accuracy: 0.2487\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 18s 269ms/step - loss: 1.9035 - accuracy: 0.2732\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 22s 336ms/step - loss: 1.8301 - accuracy: 0.2778\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 21s 310ms/step - loss: 1.7822 - accuracy: 0.2950\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 19s 285ms/step - loss: 1.7503 - accuracy: 0.2981\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 1.7108 - accuracy: 0.3101\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 19s 283ms/step - loss: 1.6854 - accuracy: 0.3009\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 19s 279ms/step - loss: 1.6601 - accuracy: 0.3251\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 18s 275ms/step - loss: 1.6491 - accuracy: 0.3289\n",
      "\n",
      "*** Validation loss: 1.626788987832911 - accuracy: 0.32309871912002563\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment('awesome_advanced_approach')\n",
    "\n",
    "with mlflow.start_run(nested=True):\n",
    "    # Log model hiperparameters first\n",
    "    mlflow.log_param('description_features_layer_size', DESCRIPTION_FEATURES_LAYER_SIZE)\n",
    "    mlflow.log_param('hidden_layer_size', HIDDEN_LAYER_SIZE)\n",
    "    mlflow.log_param('embedded_columns', embedded_columns)\n",
    "    mlflow.log_param('one_hot_columns', one_hot_columns)\n",
    "    # mlflow.log_param('numerical_columns', numerical_columns)  # Not using these yet\n",
    "    \n",
    "    # Train\n",
    "    epochs = 10\n",
    "    history = model.fit(train_dataset, epochs=epochs)\n",
    "    \n",
    "    # Evaluate\n",
    "    loss, accuracy = model.evaluate(dev_dataset, verbose=0)\n",
    "    print(\"\\n*** Validation loss: {} - accuracy: {}\".format(loss, accuracy))\n",
    "    mlflow.log_metric('epochs', epochs)\n",
    "    mlflow.log_metric('train_loss', history.history[\"loss\"][-1])\n",
    "    mlflow.log_metric('train_accuracy', history.history[\"accuracy\"][-1])\n",
    "    mlflow.log_metric('validation_loss', loss)\n",
    "    mlflow.log_metric('validation_accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluando el modelo sobre los datos de evaluación para la competencia\n",
    "\n",
    "Una vez que tenemos definido nuestro modelo, el último paso es ponerlo a prueba en los datos de evaluación para generar un archivo para enviar a la competencia Kaggle.\n",
    "\n",
    "Comenzamos cargando el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Fee</th>\n",
       "      <th>State</th>\n",
       "      <th>Description</th>\n",
       "      <th>PID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41401</td>\n",
       "      <td>I just found it alone yesterday near my apartm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41326</td>\n",
       "      <td>Their pregnant mother was dumped by her irresp...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>41326</td>\n",
       "      <td>Siu Pak just give birth on 13/6/10 to 6puppies...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41326</td>\n",
       "      <td>Very manja and gentle stray cat found, we woul...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>41326</td>\n",
       "      <td>Kali is a super playful kitten who is on the g...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Age  Breed1  Breed2  Gender  Color1  Color2  Color3  MaturitySize  \\\n",
       "0     2    1     265       0       1       1       2       0             2   \n",
       "1     1    1     307       0       1       2       7       0             2   \n",
       "2     1    0     307       0       2       1       2       7             2   \n",
       "3     2   12     265       0       2       1       7       0             2   \n",
       "4     2    3     264       0       2       1       2       5             3   \n",
       "\n",
       "   FurLength  Vaccinated  Dewormed  Sterilized  Health  Quantity  Fee  State  \\\n",
       "0          2           3         3           3       1         1    0  41401   \n",
       "1          2           1         1           2       1         1    0  41326   \n",
       "2          1           2         2           2       1         6    0  41326   \n",
       "3          2           3         3           3       1         1    0  41326   \n",
       "4          3           1         1           2       1         1   50  41326   \n",
       "\n",
       "                                         Description  PID  \n",
       "0  I just found it alone yesterday near my apartm...    1  \n",
       "1  Their pregnant mother was dumped by her irresp...    2  \n",
       "2  Siu Pak just give birth on 13/6/10 to 6puppies...    7  \n",
       "3  Very manja and gentle stray cat found, we woul...    9  \n",
       "4  Kali is a super playful kitten who is on the g...   11  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(os.path.join(DATA_DIRECTORY, 'test.csv'))\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos el conjunto de datos para darle al modelo entrenado\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Age': <tf.Tensor: id=20200, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'Breed1': <tf.Tensor: id=20201, shape=(1,), dtype=int32, numpy=array([265], dtype=int32)>,\n",
      " 'Fee': <tf.Tensor: id=20202, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
      " 'description': <tf.Tensor: id=20203, shape=(13,), dtype=int32, numpy=\n",
      "array([ 116,  429, 1371,  991,  189,    1, 7873, 1043,   62,  600,  728,\n",
      "          5,    1], dtype=int32)>,\n",
      " 'direct_features': <tf.Tensor: id=20204, shape=(10,), dtype=float32, numpy=array([1., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)>}\n",
      "\n",
      "{'Age': <tf.Tensor: id=20205, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'Breed1': <tf.Tensor: id=20206, shape=(1,), dtype=int32, numpy=array([307], dtype=int32)>,\n",
      " 'Fee': <tf.Tensor: id=20207, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
      " 'description': <tf.Tensor: id=20208, shape=(47,), dtype=int32, numpy=\n",
      "array([ 945,  154,  256, 2049,  105,  403,  991, 4678,  552,  545,    1,\n",
      "        142,  134,  403,    1,  118,  210,   73,    1,  533,  387,   35,\n",
      "        394,  272,   98,   62,    1,  464,  411,  151, 1401,   42,  253,\n",
      "          1,  825,   35, 4660,  247, 4156, 1402, 1403,    1,   43,   52,\n",
      "        599,   38,    1], dtype=int32)>,\n",
      " 'direct_features': <tf.Tensor: id=20209, shape=(10,), dtype=float32, numpy=array([1., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)>}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First tokenize the description\n",
    "\n",
    "test_dataset[\"TokenizedDescription\"] = test_dataset[\"Description\"]\\\n",
    "    .fillna(value=\"\").apply(tokenize_description)\n",
    "\n",
    "# Generate the basic TF dataset\n",
    "\n",
    "tf_test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: dataset_generator(test_dataset, True),\n",
    "    output_types=instance_types  # It should have the same instance types\n",
    ")\n",
    "\n",
    "for data in tf_test_dataset.take(2):  # The dataset only returns a data instance now (no target)\n",
    "    pprint(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tf_test_dataset.padded_batch(\n",
    "    BATCH_SIZE, \n",
    "    padded_shapes=padding_shapes[0], \n",
    "    padding_values=padding_values[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: {direct_features: (None, None), description: (None, 55), Breed1: (None, None), Age: (None, None), Fee: (None, None)}, types: {direct_features: tf.float32, description: tf.int32, Breed1: tf.int32, Age: tf.float32, Fee: tf.float32}>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correr el modelo\n",
    "\n",
    "El último paso es correr el modelo sobre los datos de evaluación para conseguir las predicciones a enviar a la competencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[\"AdoptionSpeed\"] = model.predict(test_data).argmax(axis=1)\n",
    "\n",
    "test_dataset.to_csv(\"./submission.csv\", index=False, columns=[\"PID\", \"AdoptionSpeed\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
